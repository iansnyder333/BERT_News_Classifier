{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv'\n",
    "c=pd.read_csv(url, names=['category', 'title', 'text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = {1:'politics', 2:'sport', 3:'business', 4:'tech'}\n",
    "c2 = c.replace({'category': labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                              title   \n",
       "0  business  Wall St. Bears Claw Back Into the Black (Reuters)  \\\n",
       "1  business  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2  business    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3  business  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4  business  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                                text  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  Reuters - Private investment firm Carlyle Grou...  \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3  Reuters - Authorities have halted oil export\\f...  \n",
       "4  AFP - Tearaway world oil prices, toppling reco...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "labels = {\"business\": 0, \"entertainment\": 1, \"sport\": 2, \"tech\": 3, \"politics\": 4}\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = [labels[label] for label in df[\"category\"]]\n",
    "        self.texts = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            for text in df[\"text\"]\n",
    "        ]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y\n",
    "class BertClassifier(nn.Module):\n",
    "    # Initialize BERT Classifier\n",
    "    def __init__(self, dropout=0.5):\n",
    "        # Extend the superclass for pre trained BERT Classifier\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        # Initialize the BERT model. \"bert-base-cased\" is a pre-trained BERT model, and we are using it to get the benefits of Transfer Learning.\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "        # Initialize dropout layer: a dropout layer randomly drops out (by setting to zero) a number of output features of the layer during training.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize a Linear layer: this layer combines input data into a single output through a linear transformation.\n",
    "        # The linear layer's input dimension matches the output dimension of the BERT model (768), and the output dimension is 5.\n",
    "        self.linear = nn.Linear(768, 5)\n",
    "\n",
    "        # Initialize a ReLU (Rectified Linear Unit) activation function: this function will be applied to the output of the linear layer.\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    # Define forward pass\n",
    "    def forward(self, input_id, mask):\n",
    "        # Pass the input to the BERT model. The BERT model returns the last layer's hidden-state of the first token of the sequence (CLS token) and a \"pooled\" output (an aggregation of the last layer's hidden state)\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_id, attention_mask=mask, return_dict=False\n",
    "        )\n",
    "        # Pass the \"pooled\" output through the dropout layer\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "\n",
    "        # Pass the output of the dropout layer to the linear layer\n",
    "        linear_output = self.linear(dropout_output)\n",
    "\n",
    "        # Apply the ReLU activation function to the output of the linear layer\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        # Return the output of the final layer\n",
    "        return final_layer\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs, checkpoint=None):\n",
    "    # Initialize the training and validation datasets\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    # Initialize the data loaders for the training and validation datasets\n",
    "    # The dataloaders will provide batches of data to the model during training.\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
    "\n",
    "    # Check if a GPU is available and if not, use a CPU\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # Initialize the loss function and the optimizer\n",
    "    # CrossEntropyLoss is often used in multi-class classification problems\n",
    "    # Adam is a popular choice of optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # If a GPU is available, move the model and loss function to the GPU\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    # Initialize the global step counter\n",
    "    global_step = 0\n",
    "    if checkpoint:\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        print(\"model sucsessfully loaded: \\n\")\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        # Initialize accumulators for the total training accuracy and loss\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        # Iterate over the batches of the training data loader\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            # Move the labels and inputs to the GPU if available\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_input[\"attention_mask\"].to(device)\n",
    "            input_id = train_input[\"input_ids\"].squeeze(1).to(device)\n",
    "\n",
    "            # Pass the inputs through the model\n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            # Calculate the loss of the model's predictions against the true labels\n",
    "            batch_loss = criterion(output, train_label.long())\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            # Calculate the accuracy of the model's predictions\n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            # Reset the gradients of the model parameters\n",
    "            model.zero_grad()\n",
    "            # Perform backpropagation to calculate the gradients\n",
    "            batch_loss.backward()\n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % 178 == 0:\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": epoch_num,\n",
    "                        \"model_state_dict\": model.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        \"loss\": total_loss_train,\n",
    "                    },\n",
    "                    f\"src/AI/checkpoints/checkpoint_00_{global_step}.pt\",\n",
    "                )\n",
    "\n",
    "        # Initialize accumulators for the total validation accuracy and loss\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_label in val_dataloader:\n",
    "                # Move the labels and inputs to the GPU if available\n",
    "                val_label = val_label.to(device)\n",
    "                mask = val_input[\"attention_mask\"].to(device)\n",
    "                input_id = val_input[\"input_ids\"].squeeze(1).to(device)\n",
    "\n",
    "                # Pass the inputs through the model\n",
    "                output = model(input_id, mask)\n",
    "\n",
    "                # Calculate the loss of the model's predictions against the true labels\n",
    "                batch_loss = criterion(output, val_label.long())\n",
    "                total_loss_val += batch_loss.item()\n",
    "\n",
    "                # Calculate the accuracy of the model's predictions\n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "\n",
    "            print(\n",
    "                f\"Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} | Train Accuracy: {total_acc_train / len(train_data): .3f} | Val Loss: {total_loss_val / len(val_data): .3f} | Val Accuracy: {total_acc_val / len(val_data): .3f}\"\n",
    "            )\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch_num,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"loss\": total_loss_train,\n",
    "                },\n",
    "                f\"src/AI/checkpoints/checkpoint_00_{global_step}F.pt\",\n",
    "            )\n",
    "\n",
    "def evaluate(model, test_data):\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_label in test_dataloader:\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input[\"attention_mask\"].to(device)\n",
    "            input_id = test_input[\"input_ids\"].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "\n",
    "    print(f\"Test Accuracy: {total_acc_test / len(test_data): .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.500\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "df = c2.copy()\n",
    "df = df[:1000]\n",
    "df_train, df_val, df_test = np.split(\n",
    "    df.sample(frac=1, random_state=42), [int(0.8 * len(df)), int(0.9 * len(df))]\n",
    ")\n",
    "\n",
    "\n",
    "m = BertClassifier()\n",
    "checkpoing = torch.load(\n",
    "    \"/Users/iansnyder/Desktop/Projects/NER_proj/src/AI/models/model4.pt\"\n",
    ")\n",
    "m.load_state_dict(checkpoing)\n",
    "m.eval()\n",
    "evaluate(m, df_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
